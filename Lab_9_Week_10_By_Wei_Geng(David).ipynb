{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab #9 - Week #10 By Wei Geng(David).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "njRBvvtbZcEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class IT-533 Week #10  - Applications in Data Mining: Deep Learning with TensorFlow\n",
        "# Summer 2020\n",
        "# Lab #7 - ramen-ratings.csv\n",
        "# By Wei Geng(David) and Anusha Bale\n",
        "# Created on: 07/19/2020\n",
        "# Honor Code: “I have neither given or received, nor have I tolerated others' use of unauthorized aid.”"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8npOXrx7gRIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install pandas in case you dont have pandas, matplotlib and sklearn pre-installed\n",
        "%pip install pandas\n",
        "%pip install matplotlib\n",
        "%pip sklearn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vgck51GZlu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "b1ae966d-b8b0-4227-958f-c99db49a675c"
      },
      "source": [
        "# Convert to pandas DataFrame\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "remen_df = pd.read_csv('https://raw.githubusercontent.com/intelspeedstep/AMOD533/master/ramen-ratings.csv')\n",
        "remen_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review #</th>\n",
              "      <th>Brand</th>\n",
              "      <th>Variety</th>\n",
              "      <th>Style</th>\n",
              "      <th>Country</th>\n",
              "      <th>Stars</th>\n",
              "      <th>Top Ten</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2580</td>\n",
              "      <td>New Touch</td>\n",
              "      <td>T's Restaurant Tantanmen</td>\n",
              "      <td>Cup</td>\n",
              "      <td>Japan</td>\n",
              "      <td>3.75</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2579</td>\n",
              "      <td>Just Way</td>\n",
              "      <td>Noodles Spicy Hot Sesame Spicy Hot Sesame Guan...</td>\n",
              "      <td>Pack</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2578</td>\n",
              "      <td>Nissin</td>\n",
              "      <td>Cup Noodles Chicken Vegetable</td>\n",
              "      <td>Cup</td>\n",
              "      <td>USA</td>\n",
              "      <td>2.25</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2577</td>\n",
              "      <td>Wei Lih</td>\n",
              "      <td>GGE Ramen Snack Tomato Flavor</td>\n",
              "      <td>Pack</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>2.75</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2576</td>\n",
              "      <td>Ching's Secret</td>\n",
              "      <td>Singapore Curry</td>\n",
              "      <td>Pack</td>\n",
              "      <td>India</td>\n",
              "      <td>3.75</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Review #           Brand  ... Stars Top Ten\n",
              "0      2580       New Touch  ...  3.75     NaN\n",
              "1      2579        Just Way  ...     1     NaN\n",
              "2      2578          Nissin  ...  2.25     NaN\n",
              "3      2577         Wei Lih  ...  2.75     NaN\n",
              "4      2576  Ching's Secret  ...  3.75     NaN\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNe34DsGEsSt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "068f9797-99c6-4655-fbde-b0d0f153746d"
      },
      "source": [
        "# output the data attributes and its datatypes, plus the shape of the dataset\n",
        "print(remen_df.info(verbose=True))\n",
        "print(remen_df.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2580 entries, 0 to 2579\n",
            "Data columns (total 7 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Review #  2580 non-null   int64 \n",
            " 1   Brand     2580 non-null   object\n",
            " 2   Variety   2580 non-null   object\n",
            " 3   Style     2578 non-null   object\n",
            " 4   Country   2580 non-null   object\n",
            " 5   Stars     2580 non-null   object\n",
            " 6   Top Ten   41 non-null     object\n",
            "dtypes: int64(1), object(6)\n",
            "memory usage: 141.2+ KB\n",
            "None\n",
            "(2580, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7-FWME7wXq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert Stars from object to numberic and Review # to str since ID number usually has no analytical value\n",
        "remen_df.loc[remen_df[\"Stars\"]=='Unrated','Stars']=np.nan\n",
        "remen_df['Stars']=remen_df['Stars'].astype(float)\n",
        "# remen_df['Review #']=remen_df['Review #'].astype(str)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X4BF5cCEmE-",
        "colab_type": "text"
      },
      "source": [
        "### <font color = 'orange'> Model: Build a baseline Neural Network Model using MLP with hidden_layer_sizes=(20),solver='sgd'</font>\n",
        "</font></br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29mgbebnfXOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "2bbb1022-4078-4e72-d682-6b3e931ab2f4"
      },
      "source": [
        "# Convert the string variables to numeric categorical varibles, so the model can run. Sklearn classifers requires categorical variables to be numerically coded.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "remen_df[\"Brand\"] = LabelEncoder().fit_transform(remen_df[\"Brand\"])\n",
        "remen_df[\"Variety\"] = LabelEncoder().fit_transform(remen_df[\"Variety\"])\n",
        "remen_df[\"Style\"] = LabelEncoder().fit_transform(remen_df[\"Style\"].astype(str))\n",
        "remen_df[\"Country\"] = LabelEncoder().fit_transform(remen_df[\"Country\"])\n",
        "remen_df[\"Top Ten\"] = LabelEncoder().fit_transform(remen_df[\"Top Ten\"].astype(str))\n",
        "remen_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review #</th>\n",
              "      <th>Brand</th>\n",
              "      <th>Variety</th>\n",
              "      <th>Style</th>\n",
              "      <th>Country</th>\n",
              "      <th>Stars</th>\n",
              "      <th>Top Ten</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2580</td>\n",
              "      <td>190</td>\n",
              "      <td>2189</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>3.75</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2579</td>\n",
              "      <td>119</td>\n",
              "      <td>1443</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>1.00</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2578</td>\n",
              "      <td>192</td>\n",
              "      <td>454</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>2.25</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2577</td>\n",
              "      <td>336</td>\n",
              "      <td>709</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>2.75</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2576</td>\n",
              "      <td>38</td>\n",
              "      <td>1954</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>3.75</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Review #  Brand  Variety  Style  Country  Stars  Top Ten\n",
              "0      2580    190     2189      4       18   3.75       38\n",
              "1      2579    119     1443      5       32   1.00       38\n",
              "2      2578    192      454      4       35   2.25       38\n",
              "3      2577    336      709      5       32   2.75       38\n",
              "4      2576     38     1954      5       16   3.75       38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWmuOf2td4Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using a 67% training/ 34% test split, run a Neural Network analysis on your data set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "remen_df.loc[remen_df['Stars']>3,'Stars_Ind']=1\n",
        "remen_df['Stars_Ind'].fillna(0,inplace=True)\n",
        "X=remen_df.drop(['Stars','Stars_Ind','Review #'], axis=1)\n",
        "y=remen_df['Stars_Ind']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1859, test_size=0.33)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZulPfkwwLKdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply mean normalization; this step is to standarize the features to follow the normal distribution\n",
        "# Assign the scaled data to a DataFrame & use the index and columns arguments to keep your original indices and column names:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_train_array = StandardScaler().fit_transform(X_train.values) \n",
        "X_train = pd.DataFrame(X_train_array, index=X_train.index, columns=X_train.columns)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpiA5YlzLPmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Center test data.\n",
        "X_test_array = StandardScaler().fit_transform(X_test.values)\n",
        "X_test = pd.DataFrame(X_test_array, index=X_test.index, columns=X_test.columns)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHXq-fiiLU6B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "d2dd7611-aaa9-43aa-d306-b687eba422a5"
      },
      "source": [
        "# fit the neural_network model MLP, set the hidden_layer to 10, solver='sgd',learning_rate_init=0.01,max_iter=500\n",
        "# we got a perfect model that has 100% accuracy in both training and test dataset.\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50),solver='sgd',learning_rate_init=0.001,max_iter=1000)\n",
        "mlp.fit(X_train, y_train)\n",
        "y_pred = mlp.predict(X_test)\n",
        "print(\"Test set predictions:\\n\", y_pred)\n",
        "print(\"Training set score: {:.5f}\".format(mlp.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.5f}\".format(mlp.score(X_test, y_test)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set predictions:\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Training set score: 0.77720\n",
            "Test set score: 0.76408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTw0gLZgft9k",
        "colab_type": "text"
      },
      "source": [
        "### <font color = 'orange'> Tune the model to pick the best **hyperparamenters**. This time we add more activation funcitons and solvers to auto-tune the model, and pick the best </font>\n",
        "</font></br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSPy6XycEn15",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "f93f4337-357a-4ab5-9554-ce2ba2fa7155"
      },
      "source": [
        "# GridSearchCV is a very good function where we can use to optimize the hypytermeters with crossvalidation\n",
        "# We use the additional activation function and solvers from week #10 instructor's slides\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10,10,10), (20,20,20), (20,)],\n",
        "    'activation': ['tanh', 'relu','BatchNorm3d','Conv3d','MaxPool3d'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "clf = GridSearchCV(mlp, param_grid, n_jobs=None, cv=None)\n",
        "clf.fit(X_train, y_train)\n",
        "print('Best parameters found:\\n', clf.best_params_)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The activation 'BatchNorm3d' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The activation 'Conv3d' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The activation 'MaxPool3d' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:\n",
            " {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (10, 10, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oflsmdpOfES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "12a86571-0f20-43ca-b80f-9e5d645806c7"
      },
      "source": [
        "# refit the model with the best parameters above, the performance improved.\n",
        "mlp = MLPClassifier(activation= 'tanh', alpha= 0.0001, hidden_layer_sizes= (10, 10, 10), learning_rate= 'constant', solver= 'adam',learning_rate_init=0.01,max_iter=1000)\n",
        "mlp.fit(X_train, y_train)\n",
        "y_pred = mlp.predict(X_test)\n",
        "print(\"Test set predictions:\\n\", y_pred)\n",
        "print(\"Training set score: {:.5f}\".format(mlp.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.5f}\".format(mlp.score(X_test, y_test)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set predictions:\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
            " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
            " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
            "Training set score: 0.82118\n",
            "Test set score: 0.73239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhAFzZ0J9mpR",
        "colab_type": "text"
      },
      "source": [
        "#### <font color = 'dark red'> After auto-tune the model with more activation functions and solvers that we learn in week 10, Anusha and I were able to improve the model accuracy from 0.77720 to 0.82118 on the training dataset. Even though the model has some overfitting issue, this is a hug improvement as we get 5% improvement, and we are very please to see the results and to see how powerful those additional methods can improve the neural network model. </font>\n",
        "</font></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL4-iAvkiWpN",
        "colab_type": "text"
      },
      "source": [
        "### <font color = 'orange'>------------------------------------------------------------------------END--------------------------------------------------------------------------------- </font>"
      ]
    }
  ]
}